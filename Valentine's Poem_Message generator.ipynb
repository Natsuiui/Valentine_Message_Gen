{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5031d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b86c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1647133 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('ValentinePoemData.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fcbdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Valentineâ€™s Day to the most special person in my life.\n",
      "\n",
      "I love you more than pizza.\n",
      "\n",
      "Here's to being my emergency contact someday.\n",
      "\n",
      "You're my everything. Happy Valentine's Day!\n",
      "\n",
      "Are you a banana? Because I find you a-peel-ing.\n",
      "\n",
      "Happy Valentine'\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a37ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac67a626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd98ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8ffb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[36, 37, 38, 39, 40, 41, 42], [59, 60, 61]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6312f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8e663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c7d0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413ed9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a651f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1647133,), dtype=int64, numpy=array([20, 36, 51, ..., 36, 60,  3])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db6bb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a282d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "a\n",
      "p\n",
      "p\n",
      "y\n",
      " \n",
      "V\n",
      "a\n",
      "l\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "n\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(15):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa52b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6510d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'H' b'a' b'p' b'p' b'y' b' ' b'V' b'a' b'l' b'e' b'n' b't' b'i' b'n'\n",
      " b'e' b'\\xe2\\x80\\x99' b's' b' ' b'D' b'a' b'y' b' ' b't' b'o' b' ' b't'\n",
      " b'h' b'e' b' ' b'm' b'o' b's' b't' b' ' b's' b'p' b'e' b'c' b'i' b'a'\n",
      " b'l' b' ' b'p' b'e' b'r' b's' b'o' b'n' b' ' b'i' b'n' b' ' b'm' b'y'\n",
      " b' ' b'l' b'i' b'f' b'e' b'.' b'\\n' b'\\n' b'I' b' ' b'l' b'o' b'v' b'e'\n",
      " b' ' b'y' b'o' b'u' b' ' b'm' b'o' b'r' b'e' b' ' b't' b'h' b'a' b'n'\n",
      " b' ' b'p' b'i' b'z' b'z' b'a' b'.' b'\\n' b'\\n' b'H' b'e' b'r' b'e' b\"'\"\n",
      " b's' b' ' b't' b'o' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb20169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Happy Valentine\\xe2\\x80\\x99s Day to the most special person in my life.\\n\\nI love you more than pizza.\\n\\nHere's to \"\n",
      "b\"being my emergency contact someday.\\n\\nYou're my everything. Happy Valentine's Day!\\n\\nAre you a banana? \"\n",
      "b\"Because I find you a-peel-ing.\\n\\nHappy Valentine's Day, handsome.\\n\\nYou're the only person I send heart\"\n",
      "b' eye emojis to.\\n\\nValentine, you take my breath away, every single day.\\n\\nIt\\xe2\\x80\\x99s just one day in the year'\n",
      "b', but you should know that I love you every day and every moment.\\n\\nThe more time we spend together, t'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ef6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb6bf724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "828cb8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be910bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"Happy Valentine\\xe2\\x80\\x99s Day to the most special person in my life.\\n\\nI love you more than pizza.\\n\\nHere's to\"\n",
      "Target: b\"appy Valentine\\xe2\\x80\\x99s Day to the most special person in my life.\\n\\nI love you more than pizza.\\n\\nHere's to \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 16:31:46.968859: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92a16bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3232232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45ec87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499462f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96fe0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 64) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18b8684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16384     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  65600     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,020,288\n",
      "Trainable params: 4,020,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa1013f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0493aa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 56, 47, 51, 53, 51,  0,  2,  7, 29,  9, 54, 20, 19, 30, 20, 30,\n",
       "       18,  2, 31, 53, 58, 20, 16,  9, 38, 12, 48,  1,  9, 29, 32, 38, 55,\n",
       "       33,  2, 63,  1, 47,  8, 32,  6, 13, 51, 52, 49, 14, 44, 13,  6, 16,\n",
       "       28, 18, 48,  3, 54,  4, 13, 12, 47, 30, 53, 58, 16,  1, 15, 30, 59,\n",
       "       60, 30, 36, 13, 22,  6, 35, 21,  2, 35, 36,  5, 51, 27, 43, 32, 24,\n",
       "        1, 46, 62, 41, 36, 15, 57, 15, 35, 20, 26, 31, 18,  4, 53])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba9152d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"Day, handsome.\\n\\nYou're the only person I send heart eye emojis to.\\n\\nValentine, you take my breath aw\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"mulprp[UNK] ,R.sHGSHSF TrwHD.c?m\\n.RUctV \\xe2\\x80\\x99\\nl-U)ApqnBiA)DPFm!s'A?lSrwD\\nCSxySaAJ)YI Ya(pOhUL\\nk\\xe2\\x80\\x94faCvCYHNTF'r\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d33ed081",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e52c9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 64)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.158709, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8616c10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.98886"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c20c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28f1a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b9b19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e57abe8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "254/254 [==============================] - 186s 729ms/step - loss: 2.2270\n",
      "Epoch 2/50\n",
      "254/254 [==============================] - 213s 836ms/step - loss: 0.5880\n",
      "Epoch 3/50\n",
      "254/254 [==============================] - 241s 950ms/step - loss: 0.1352\n",
      "Epoch 4/50\n",
      "254/254 [==============================] - 241s 946ms/step - loss: 0.1106\n",
      "Epoch 5/50\n",
      "254/254 [==============================] - 232s 913ms/step - loss: 0.1008\n",
      "Epoch 6/50\n",
      "254/254 [==============================] - 256s 1s/step - loss: 0.0948\n",
      "Epoch 7/50\n",
      "254/254 [==============================] - 261s 1s/step - loss: 0.0912\n",
      "Epoch 8/50\n",
      "254/254 [==============================] - 273s 1s/step - loss: 0.0889\n",
      "Epoch 9/50\n",
      "254/254 [==============================] - 278s 1s/step - loss: 0.0868\n",
      "Epoch 10/50\n",
      "254/254 [==============================] - 230s 904ms/step - loss: 0.0855\n",
      "Epoch 11/50\n",
      "254/254 [==============================] - 243s 955ms/step - loss: 0.0845\n",
      "Epoch 12/50\n",
      "254/254 [==============================] - 247s 971ms/step - loss: 0.0834\n",
      "Epoch 13/50\n",
      "254/254 [==============================] - 248s 975ms/step - loss: 0.0827\n",
      "Epoch 14/50\n",
      "254/254 [==============================] - 248s 976ms/step - loss: 0.0818\n",
      "Epoch 15/50\n",
      "254/254 [==============================] - 251s 986ms/step - loss: 0.0815\n",
      "Epoch 16/50\n",
      "254/254 [==============================] - 250s 984ms/step - loss: 0.0810\n",
      "Epoch 17/50\n",
      "254/254 [==============================] - 250s 981ms/step - loss: 0.0804\n",
      "Epoch 18/50\n",
      "254/254 [==============================] - 248s 977ms/step - loss: 0.0801\n",
      "Epoch 19/50\n",
      "254/254 [==============================] - 249s 979ms/step - loss: 0.0797\n",
      "Epoch 20/50\n",
      "254/254 [==============================] - 250s 982ms/step - loss: 0.0794\n",
      "Epoch 21/50\n",
      "254/254 [==============================] - 250s 984ms/step - loss: 0.0791\n",
      "Epoch 22/50\n",
      "254/254 [==============================] - 250s 983ms/step - loss: 0.0786\n",
      "Epoch 23/50\n",
      "254/254 [==============================] - 252s 989ms/step - loss: 0.0785\n",
      "Epoch 24/50\n",
      "254/254 [==============================] - 251s 986ms/step - loss: 0.0781\n",
      "Epoch 25/50\n",
      "254/254 [==============================] - 227s 893ms/step - loss: 0.0779\n",
      "Epoch 26/50\n",
      "254/254 [==============================] - 203s 799ms/step - loss: 0.0778\n",
      "Epoch 27/50\n",
      "254/254 [==============================] - 246s 968ms/step - loss: 0.0773\n",
      "Epoch 28/50\n",
      "254/254 [==============================] - 204s 801ms/step - loss: 0.0768\n",
      "Epoch 29/50\n",
      "254/254 [==============================] - 209s 822ms/step - loss: 0.0766\n",
      "Epoch 30/50\n",
      "254/254 [==============================] - 224s 881ms/step - loss: 0.0764\n",
      "Epoch 31/50\n",
      "254/254 [==============================] - 221s 867ms/step - loss: 0.0763\n",
      "Epoch 32/50\n",
      "254/254 [==============================] - 221s 868ms/step - loss: 0.0761\n",
      "Epoch 33/50\n",
      "254/254 [==============================] - 218s 858ms/step - loss: 0.0758\n",
      "Epoch 34/50\n",
      "254/254 [==============================] - 224s 881ms/step - loss: 0.0757\n",
      "Epoch 35/50\n",
      "254/254 [==============================] - 218s 856ms/step - loss: 0.0754\n",
      "Epoch 36/50\n",
      "254/254 [==============================] - 220s 865ms/step - loss: 0.0752\n",
      "Epoch 37/50\n",
      "254/254 [==============================] - 252s 990ms/step - loss: 0.0751\n",
      "Epoch 38/50\n",
      "254/254 [==============================] - 245s 963ms/step - loss: 0.0747\n",
      "Epoch 39/50\n",
      "254/254 [==============================] - 239s 939ms/step - loss: 0.0746\n",
      "Epoch 40/50\n",
      "254/254 [==============================] - 231s 908ms/step - loss: 0.0743\n",
      "Epoch 41/50\n",
      "254/254 [==============================] - 244s 958ms/step - loss: 0.0742\n",
      "Epoch 42/50\n",
      "254/254 [==============================] - 247s 972ms/step - loss: 0.0740\n",
      "Epoch 43/50\n",
      "254/254 [==============================] - 289s 1s/step - loss: 0.0740\n",
      "Epoch 44/50\n",
      "254/254 [==============================] - 226s 886ms/step - loss: 0.0743\n",
      "Epoch 45/50\n",
      "254/254 [==============================] - 214s 839ms/step - loss: 0.0741\n",
      "Epoch 46/50\n",
      "254/254 [==============================] - 210s 827ms/step - loss: 0.0735\n",
      "Epoch 47/50\n",
      "254/254 [==============================] - 211s 830ms/step - loss: 0.0732\n",
      "Epoch 48/50\n",
      "254/254 [==============================] - 210s 824ms/step - loss: 0.0732\n",
      "Epoch 49/50\n",
      "254/254 [==============================] - 211s 829ms/step - loss: 0.0731\n",
      "Epoch 50/50\n",
      "254/254 [==============================] - 201s 790ms/step - loss: 0.0732\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7198b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    " \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "        return predicted_chars, states\n",
    "    \n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19bc2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first phrase :) - Adya, i love you. You are perfect.\n",
      "Enter number of characters - 1000\n",
      " \n",
      "Adya, i love you. You are perfect.\n",
      "\n",
      "I desire you my love in every moment.\n",
      "As time adds years like sands on the beach,\n",
      "When you sense your beauty slipping to the wind,\n",
      "I will want you so much more than now.\n",
      "\n",
      "I could watch the sunrise, I could watch the sunset.\n",
      "I could walk in the rain without getting wet.\n",
      "I could witness their beauty as the flowers bloom.\n",
      "I could I could listen to bird songs as the new day dawns.\n",
      "I could feel the sunshine through the darkest clouds.\n",
      "I could hear a sweet whisper through the roar of a crowd.\n",
      "I could walk on water without soaking my feet.\n",
      "I could travel the world in less than a week.\n",
      "I could ponder the knowledge passed down by the wise.\n",
      "I could dwell in a castle built in the sky.\n",
      "I could paint a portrait and bring it to life.\n",
      "I could sleep without pain, on the edge of a knife.\n",
      "I could play a sweet melody to soften the mood.\n",
      "I could take all the bad and turn it to good.\n",
      "I could float on the breeze, I could fly on the wind.\n",
      "I could soar high in the sky, leaving the world behind.\n",
      "I could trav \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.5524606704711914\n"
     ]
    }
   ],
   "source": [
    "inputstring = input(\"Enter the first phrase :) - \")\n",
    "lengths = int(input(\"Enter number of characters - \"))\n",
    "print(\" \")\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([inputstring])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(lengths):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76222ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x2b309d570>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47334831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adya ust of your kiss.\n",
      "\n",
      "I desire you when the sun centers the sky\n",
      "And shadows hide beneath the soles of feet.\n",
      "When butterflies dance among petals white,\n",
      "I want the sparkle of your smiling eyes.\n",
      "\n",
      "I desire you as the moon welcomes the night\n",
      "And skies are flooded with twinkling bright stars.\n",
      "When candles flicker their last silent breath,\n",
      "I then want to feel your sensuous touch.\n",
      "\n",
      "I desire you when passion swells all your soul\n",
      "And your lovely face shines with radiance.\n",
      "When your moist lips are full of love's desire,\n",
      "I want to feel every pulse of your heart.\n",
      "\n",
      "I desire you when you are weary and down\n",
      "And gray skies sprinkle droplets of sad tears.\n",
      "When the world laughs at your every stumble,\n",
      "I want to gently sooth your sorrows.\n",
      "\n",
      "I desire you my love in every moment.\n",
      "As time adds years like sands on the beach,\n",
      "When you sense your beauty slipping to the wind,\n",
      "I will want you so much more than now.\n",
      "\n",
      "I could watch the sunrise, I could watch the sunset.\n",
      "I could walk in the rain without getting wet.\n",
      "I co\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['Adya '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefa88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
